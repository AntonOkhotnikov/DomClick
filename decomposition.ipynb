{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, NMF, LatentDirichletAllocation\n",
    "import h5py\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can open here or BOW encoding 'embedded_bow.h5' or TF-IDF encoding 'embedded_tfidf.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10868, 10861)\n",
      "(10868,)\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "filename = '/data/embedded_tfidf.h5'  # embedded_bow.h5 - BOW encoding, embedded_tfidf.h5 - TFIDF\n",
    "\n",
    "with h5py.File(path + filename, 'r') as h5file:\n",
    "    X_train = h5file['train'][:]\n",
    "    Y_train = h5file['target'][:]\n",
    "    h5file.close()\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by using the PCA, SVD (NMF and LDA are computationally expensive and don't give any improvement of performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVD\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_SVD = svd.fit_transform(X_train)\n",
    "X_train_SVD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "# nmf = NMF(n_components=10, random_state=42)\n",
    "# X_train_NMF = nmf.fit_transform(X_train)\n",
    "# X_train_NMF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "# LDir = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "# X_train_LDA = LDir.fit_transform(X_train)\n",
    "# X_train_LDA.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, model):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    Y_pred = model.predict_proba(X_test)\n",
    "    print('AUC', roc_auc_score(y_test, Y_pred[:, 1]))\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print('Accuracy', accuracy_score(y_test, Y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the performance with Logistic Regression and Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD decomposition\n",
      "AUC 0.9121861281826162\n",
      "Accuracy 0.8467198038013488\n",
      "---------\n",
      "AUC 0.9089103502097355\n",
      "Accuracy 0.8626609442060086\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lr = LR()\n",
    "\n",
    "print('SVD decomposition')\n",
    "train_model(X_train_SVD, Y_train, lr)\n",
    "print('---------')\n",
    "train_model(X_train_SVD, Y_train, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA decomposition\n",
      "AUC 0.9121022339283973\n",
      "Accuracy 0.8467198038013488\n",
      "---------\n",
      "AUC 0.9078548434299092\n",
      "Accuracy 0.8608215818516247\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lr = LR()\n",
    "\n",
    "print('PCA decomposition')\n",
    "train_model(X_train_PCA, Y_train, lr)\n",
    "print('---------')\n",
    "train_model(X_train_PCA, Y_train, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = LDA()\n",
    "# lr = LR()\n",
    "# \n",
    "# train_model(X_train_NMF, Y_train, lr)\n",
    "# print('---------')\n",
    "# train_model(X_train_NMF, Y_train, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = LDA()\n",
    "# lr = LR()\n",
    "\n",
    "# train_model(X_train_LDA, Y_train, lr)\n",
    "# print('---------')\n",
    "# train_model(X_train_LDA, Y_train, lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM - based feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is taken from https://gist.github.com/danoneata/9927923\n",
    "def to_fv(row):\n",
    "    global gmm\n",
    "    xx = np.atleast_2d(row)\n",
    "    N = xx.shape[0]\n",
    "\n",
    "    # Compute posterior probabilities\n",
    "    Q = gmm.predict_proba(xx)  # NxK\n",
    "\n",
    "    # Compute the sufficient statistics of descriptors\n",
    "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
    "    Q_xx = np.dot(Q.T, xx) / N\n",
    "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
    "\n",
    "    # Compute derivatives with respect to\n",
    "    # mixing weights, means and variances\n",
    "    d_pi = Q_sum.squeeze() - gmm.weights_\n",
    "    d_mu = Q_xx - Q_sum * gmm.means_\n",
    "    d_sigma = ( - Q_xx_2 - Q_sum * gmm.means_ ** 2 + Q_sum * gmm.covariances_ + 2 * Q_xx * gmm.means_)\n",
    "\n",
    "    # Merge derivatives into a\n",
    "    # vector.\n",
    "    return np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assumption here is that the data came from the mixture of two Gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 402)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 2  # number of GMM componenets\n",
    "gmm = GaussianMixture(n_components=K, covariance_type='diag', reg_covar=1e-4)\n",
    "gmm.fit(X_train_SVD)\n",
    "\n",
    "X_train_GMM = np.array([to_fv(X_train_SVD[row, :]) for row in range(X_train_SVD.shape[0])])\n",
    "X_train_GMM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM features\n",
      "AUC 0.9063779143498196\n",
      "Accuracy 0.8393623543838136\n",
      "---------\n",
      "AUC 0.9080772607550482\n",
      "Accuracy 0.8583690987124464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ao2u17/.conda/envs/august/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lr = LR()\n",
    "\n",
    "print('GMM features')\n",
    "train_model(X_train_GMM, Y_train, lr)\n",
    "print('---------')\n",
    "train_model(X_train_GMM, Y_train, lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can save here X_train_SVD, X_train_PCA or X_train_GMM with a appropriate name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/decomposed_tfidf_GMM.h5', 'w') as h5file:  # decomposed_BOW_{decomposition}.h5 if BOW encoding\n",
    "    h5file.create_dataset('train', data=X_train_GMM)\n",
    "    h5file.create_dataset('target', data=Y_train)\n",
    "    h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
